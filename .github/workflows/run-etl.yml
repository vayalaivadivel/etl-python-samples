name: Run PySpark ETL

on:
  workflow_dispatch:

jobs:
  deploy-etl:
    runs-on: ubuntu-latest

    env:
      EC2_IP: ${{ secrets.EC2_PUBLIC_IP }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Setup SSH key
      uses: webfactory/ssh-agent@v0.9.0
      with:
        ssh-private-key: ${{ secrets.EC2_PRIVATE_KEY }}

    - name: Copy ETL project to EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@$EC2_IP "mkdir -p ~/pyspark-etl-project"
        scp -o StrictHostKeyChecking=no -r pyspark-etl-project/etl ubuntu@$EC2_IP:~/pyspark-etl-project/

    - name: Run PySpark ETL on EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@$EC2_IP << 'EOF'
          set -e
          cd ~/pyspark-etl-project/etl

          echo "Running spark-submit..."
          spark-submit \
            --packages org.apache.hadoop:hadoop-aws:3.3.2,mysql:mysql-connector-java:8.0.33 \
            src/load_s3_to_rds.py

          echo "âœ… ETL JOB COMPLETED SUCCESSFULLY"
        EOF