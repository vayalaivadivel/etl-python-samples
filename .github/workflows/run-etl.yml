name: Run PySpark ETL

on:
  workflow_dispatch:

jobs:
  deploy-etl:
    runs-on: ubuntu-latest

    env:
      EC2_IP: ${{ secrets.EC2_PUBLIC_IP }}

    steps:
    # 1️⃣ Checkout the repo
    - name: Checkout repository
      uses: actions/checkout@v3

    # 2️⃣ Setup SSH agent for EC2
    - name: Setup SSH key
      uses: webfactory/ssh-agent@v0.9.0
      with:
        ssh-private-key: ${{ secrets.EC2_PRIVATE_KEY }}

    # 3️⃣ Copy ETL project to EC2
    - name: Copy ETL project to EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@$EC2_IP "mkdir -p ~/pyspark-etl-project"
        scp -o StrictHostKeyChecking=no -r pyspark-etl-project/* ubuntu@$EC2_IP:~/pyspark-etl-project/

    # 4️⃣ Run ETL script on EC2
    - name: Run ETL on EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@$EC2_IP << 'EOF'
          set -e
          # Navigate to etl folder where run_etl.sh is located
          cd ~/pyspark-etl-project/etl

          # Make sure the ETL runner is executable
          chmod +x run_etl.sh

          # Execute the ETL script
          ./run_etl.sh
        EOF