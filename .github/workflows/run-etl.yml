name: Run PySpark ETL

on:
  workflow_dispatch:

jobs:
  deploy-etl:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Setup SSH key
      uses: webfactory/ssh-agent@v0.9.0
      with:
        ssh-private-key: ${{ secrets.EC2_PRIVATE_KEY }}

    - name: Copy ETL project to EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@${{ secrets.EC2_PUBLIC_IP }} \
          "mkdir -p ~/pyspark-etl-project"

        scp -o StrictHostKeyChecking=no -r pyspark-etl-project/etl \
          ubuntu@${{ secrets.EC2_PUBLIC_IP }}:~/pyspark-etl-project/

    - name: Install Python dependencies (if needed)
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@${{ secrets.EC2_PUBLIC_IP }} \
        "pip3 install --user -r ~/pyspark-etl-project/etl/requirements.txt"

    - name: Run PySpark ETL on EC2
      run: |
        ssh -o StrictHostKeyChecking=no ubuntu@${{ secrets.EC2_PUBLIC_IP }} << 'EOF'
          set -e   # stop if any command fails

          cd ~/pyspark-etl-project/etl

          spark-submit \
            --packages org.apache.hadoop:hadoop-aws:3.3.2,mysql:mysql-connector-java:8.0.33 \
            etl.py

          echo "ETL JOB COMPLETED SUCCESSFULLY"
        EOF