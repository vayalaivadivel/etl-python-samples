name: Run PySpark ETL

on:
  workflow_dispatch:

jobs:
  deploy-etl:
    runs-on: ubuntu-latest

    env:
      EC2_IP: ${{ secrets.EC2_PUBLIC_IP }}
      EC2_USER: ubuntu
      PROJECT_PATH: ~/pyspark-etl-project

    steps:
      # 1️⃣ Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2️⃣ Setup SSH key for EC2 access
      - name: Setup SSH key
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.EC2_PRIVATE_KEY }}

      # 3️⃣ Create project directory on EC2 & copy files
      - name: Copy ETL project to EC2
        run: |
          ssh -o StrictHostKeyChecking=no $EC2_USER@$EC2_IP "mkdir -p $PROJECT_PATH"
          scp -o StrictHostKeyChecking=no -r pyspark-etl-project/* $EC2_USER@$EC2_IP:$PROJECT_PATH/

      # 4️⃣ Run the ETL job
      - name: Run ETL script on EC2
        run: |
          ssh -o StrictHostKeyChecking=no $EC2_USER@$EC2_IP << 'EOF'
            set -e
            cd ~/pyspark-etl-project/etl
            chmod +x run_etl.sh
            ./run_etl.sh
          EOF

      # 5️⃣ (Optional) Fetch ETL logs back to GitHub runner
      - name: Fetch ETL logs
        run: |
          mkdir -p ./logs
          scp -o StrictHostKeyChecking=no $EC2_USER@$EC2_IP:$PROJECT_PATH/etl/logs/* ./logs/